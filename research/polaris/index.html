<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="PoLaRIS Dataset: A Maritime Object Detection and Tracking Dataset in Pohang Canal">
  <meta name="keywords" content="Maritime Dataset Object Detection Tracking">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>PoLaRIS Dataset: A Maritime Object Detection and Tracking Dataset in Pohang Canal</title>

  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/logo.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">PoLaRIS Dataset: A Maritime Object Detection and Tracking Dataset in Pohang Canal</h1>
          <div class="is-size-6 publication-authors">
            <span class="author-block">IEEE International Conference on Robotics and Automation (ICRA) 2025</span>            
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=wL8VdUMAAAAJ&hl=ko">Jiwon Choi</a><sup>1*</sup>,</span>
            <span class="author-block">
                <a href="https://scholar.google.com/citations?hl=ko&user=dE64iRYAAAAJ&view_op=list_works&gmla=ALUCkoU7OqZO7msra3b_HIxOpD8QkMZcRwBNfei8u_UFen_0ZYI9C0xjKxgISBUOy7whOJOiLSmzX5hz5I_AbI0L5Y8G6UCT21AGalln_K1gPzM32FY6wmlH3g">Dongjin Cho</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=iKsImcYAAAAJ&hl=ko">Gihyeon Lee</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=t5UEbooAAAAJ&hl=ko">Hogyun Kim</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=UPg-JuQAAAAJ&hl=ko">Geonmo Yang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              Joowan Kim</a><sup>2</sup>,
            </span>
            <span class="author-block">
              and <a href="https://scholar.google.com/citations?user=9mKOLX8AAAAJ&hl=ko">Younggun Cho</a><sup>1</sup>
            </span>            
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Spatial AI and Robotics (SPARO) Lab, Inha University, South Korea</span>            
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>2</sup> Samsung Heavy Industry, Daejeon, South Korea</span>            
          </div>

          <div class="is-size-6 contribution">
            <span class="author-block">(*) represents equal contribution.</span>            
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2412.06192"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2412.06192"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=26BinQIRc78"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/sparolab/PoLaRIS"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://docs.google.com/forms/d/e/1FAIpQLSeXqji_9KVj6SEk3lYVewzi_krMeQNNen2r2E9ZCAwKhPfNwQ/viewform"
                   class="external-link button is-normal is-rounded is-dark" target="_blank">
                  <span class="icon">
                    <i class="fas fa-download"></i>
                  </span>
                  <span>Dataset Download</span>
                </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="video-container">
    <video id="fast_lio_sam_qn" autoplay controls muted loop playsinline 
           style="max-width: 700px; width: 100%; height: auto;" class="reduced-video">
      <source src="./static/videos/data_set.mp4" type="video/mp4">
    </video>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Maritime environments often present hazardous situations due to factors such as moving ships or buoys, which become obstacles under the influence of waves. In such challenging conditions, the ability to detect and track potentially hazardous objects is critical for the safe navigation of marine robots. To address the scarcity of comprehensive datasets capturing these dynamic scenarios, we introduce a new multimodal dataset that includes image and point-wise annotations of maritime hazards. 
            Our dataset provides detailed ground truth for obstacle detection and tracking, including objects as small as 10×10 pixels, which are crucial for maritime safety. To validate the dataset’s effectiveness as a reliable benchmark, we conducted evaluations using various methodologies, including state-of-the-art (SOTA) techniques for object detection and tracking. These evaluations are expected to contribute to performance improvements, particularly in the complex maritime environment.
            To the best of our knowledge, this is the first dataset offering multi-modal annotations specifically tailored to maritime environments.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<section class="section">
  <div class="container is-max-desktop">

    <!-- Introduction -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-2" id="intro">Why Polaris-Dataset?</h2> <!-- 왼쪽 정렬 -->
        <div class="content has-text-justified">
          <p>
            Unmanned Surface Vehicles (USVs) require precise object recognition for safe navigation but face challenges such as irregular lighting and unpredictable obstacles.
            However, existing maritime datasets have two major limitations:
          </p>

          <ul>
            <li><strong>Sensor Reliability:</strong> Poor performance under challenging lighting, with limited multi-sensor integration.</li>
            <li><strong>Dynamic Object Tracking:</strong> Insufficient tracking of long-range or small objects, critical for USV collision avoidance.</li>
          </ul>

          <p>
            To address these issues, <strong>Polaris-Dataset</strong> introduces the following key contributions:
          </p>

          <ul>
            <li><strong>Multi-Scale Object Annotation:</strong> Manual refinement of large and small object annotations, initially generated by an object detector.</li>
            <li><strong>Dynamic Object Tracking:</strong> Comprehensive tracking annotations to enhance navigation performance.</li>
            <li><strong>Multi-Modal Annotations:</strong> Stereo RGB, Thermal Infrared (TIR), LiDAR, and Radar data annotated using a semi-automatic, human-verified process.</li>
            <li><strong>Benchmark Validation:</strong> Evaluations with conventional and SOTA methods demonstrate the dataset's effectiveness in maritime environments.</li>
          </ul>

          <div class="has-text-centered mt-5">
            <img src="./static/images/pipeline.png" alt="Polaris-Dataset Pipeline" style="max-width: 700px; width: 100%; height: auto;">
          </div>
        </div>
      </div>
    </div>

    <hr>

    <!-- Image Annotation -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-4" id="image-annotation">Image Annotation Process</h2> <!-- 왼쪽 정렬 -->
        
        <div class="has-text-centered mb-5">
          <img src="./static/images/image_labeling.png" alt="Image Annotation Process" style="max-width: 915px; width: 100%; height: auto;">
        </div>

        <div class="content has-text-justified">
          <h3 class="title is-5">Stereo Left Camera (RGB)</h3>

          <h4 class="title is-6">Daytime</h4>
          <ul>
            <li>One image is manually sampled every 20 images to create a training set.</li>
            <li>Remaining images are initialized using YOLOv8 detections, with overlapping boxes (IoU ≥ 0.8) removed.</li>
            <li>All small objects (occupying less than 5% of the image size) are manually annotated.</li>
          </ul>

          <h4 class="title is-6">Nighttime</h4>
          <ul>
            <li>Low-light images are enhanced using a diffusion-based GSAD method instead of relying on noisy TIR images.</li>
            <li>Manual annotations are performed on the enhanced images.</li>
          </ul>

          <h3 class="title is-5">Stereo Right Camera (RGB)</h3>
          <ul>
            <li>Initial bounding boxes are transferred from the left image annotations.</li>
            <li>Manual corrections are applied to account for differences in field of view (FOV).</li>
          </ul>
        </div>
      </div>
    </div>

    <hr>

    <!-- Semi-Automatic Annotation -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-4" id="semi-auto">Semi-Automatic Annotation</h2> <!-- 왼쪽 정렬 -->

        <div class="has-text-centered mb-5">
          <img src="./static/images/semi_auto_labeling.png" alt="Semi-Automatic Annotation" style="max-width: 900px; width: 100%; height: auto;">
        </div>

        <div class="content has-text-justified">
          <h3 class="title is-5">TIR Images</h3>
          <ul>
            <li>Annotations are transferred from the left camera using extrinsic parameters.</li>
            <li>Transformation formula:</li>
            <div class="has-text-centered my-4">
              <img src="./static/images/semi_auto_metric.png" alt="Transformation Formula" style="max-width: 200px; width: 100%; height: auto;">
            </div>
            <li>Manual corrections are required to address FOV differences and transformation errors.</li>
            <li>16-bit TIR images are converted to 8-bit using Fieldscale for better visualization before manual refinement.</li>
          </ul>

          <h3 class="title is-5">LiDAR Points</h3>
          <ul>
            <li>LiDAR points are projected onto the left image based on extrinsic calibration.</li>
            <li>Points outside the bounding boxes are filtered out.</li>
            <li>Manual verification ensures accurate object shape representation.</li>
          </ul>

          <h3 class="title is-5">Radar Points</h3>
          <ul>
            <li>Radar annotations are generated through fusion with LiDAR data.</li>
            <li>The annotation process includes:
              <ul>
                <li>Converting LiDAR-labeled points into Radar BEV coordinates.</li>
                <li>Clustering Radar points using the DBSCAN algorithm.</li>
                <li>Assigning annotations by matching Radar clusters with labeled LiDAR points.</li>
              </ul>
            </li>
            <li>This approach reflects Radar's characteristics and enhances annotation accuracy.</li>
          </ul>
        </div>
      </div>
    </div>

  </div>
</section>
<hr>

<section class="section">
  <div class="container is-max-desktop">

    <!-- Introduction Title -->
    <div class="columns">
      <div class="column is-full-width">
        <h2 class="title is-2" id="intro">About Polaris-Dataset</h2>
      </div>
    </div>

    <!-- Ground Truth Trajectory Section -->
    <div class="columns is-vcentered">
      
      <!-- Left side: Text -->
      <div class="column is-half">
        <h3 class="title is-4">Ground Truth Trajectory</h3>
        <div class="content">
          <ul>
            <li><em>All the ground truth trajectories are from the</em> 
              <a href="https://sites.google.com/view/pohang-canal-dataset/home" target="_blank"><strong>Pohang Canal Dataset</strong></a>.
            </li>
            <li>The trajectory of all data sequences consists of (a) to (d), and there are a total of 6 sequences.</li>
            <li>We selected five major sequences 
              <strong>Pohang00, 02, 03, 04</strong> for day and 
              <strong>Pohang01</strong> for night.
              Pohang05 was excluded due to the absence of dynamic objects and poor lighting conditions.
            </li>
          </ul>
        </div>
      </div>

      <!-- Right side: Image -->
      <div class="column is-half">
        <div class="has-text-centered">
          <figure class="image">
            <img src="./static/images/Gt_trajectoy.png" alt="Dataset Overview" style="max-width: 100%; height: auto;">
            <!-- <figcaption class="has-text-centered is-size-7 mt-2">Map and Trajectory</figcaption> -->
          </figure>
        </div>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-4" id="semi-auto">Data Fomat</h2>

        <div class="has-text-centered mb-5">
          <img src="./static/images/Data_format.png" alt="Semi-Automatic Annotation" style="max-width: 500px; width: 100%; height: auto;">
        </div>
        <div class="content">
          <ul>
            <li>All <strong>label files for the image sensors </strong> are stored in a folder named all.</li>
            <li>All <strong>image labels and point-wise labels</strong> of dynamic objects store in the folder named dynamic.</li>
            </ul>
        </div>
      </div>
    </div>


    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-4" id="semi-auto">Image Annotation Format</h2>

        <div class="has-text-centered mb-5">
          <img src="./static/images/Image_annotation_format.png" alt="Dynamic image Annotation Format" style="max-width: 500px; width: 100%; height: auto;">
        </div>
        <div class="content">
          <ul>
            <li>All image annotations are stored object bounding boxes for the Stereo and TIR cameras, and are in YOLO format.</li>
          </ul>
        </div>
      </div>
    </div>
  
    
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-4" id="semi-auto">Dynamic image Annotation Format</h2>
        <div class="has-text-centered mb-5">
          <img src="./static/images/Dynamic_annotation_format.png" alt="Dynamic image Annotation Format" style="max-width: 600px; width: 100%; height: auto;">
        </div>
        <div class="content">
          <ul>
            <li>The dynamic image annotations are stored in YOLO format, including the object IDs corresponding to the left image.</li>
          </ul>
        </div>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-4" id="semi-auto">Dynamic Point-wise Annotation Format</h2>
        <div class="content">
          <ul>
            <li>The dynamic pose-wise annotations are stored the LiDAR and radar PCD format corresponding to the object ID.</li>
          </ul>
        </div>
      </div>
    </div>
    
  </div>
</div>
</section>
<hr>
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns">
      <div class="column is-full-width">
        <h2 class="title is-2" id="intro">Polaris Dataset Downloads</h2>
      </div>
    </div>
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content">
          <ul>
            <li><strong>All sequences are based on the</strong> 
              <a href="https://sites.google.com/view/pohang-canal-dataset/home" target="_blank"><strong>Pohang Canal Dataset</strong></a>.
            </li>
            <li><strong>Also, the <span style="color:green;">Ground-Truth pose for all detection and tracking data</span></strong> exists in the 
              <a href="https://sites.google.com/view/pohang-canal-dataset/home" target="_blank"><strong>Pohang Canal Dataset</strong></a>.
            </li>
            <li><em>Since each sequence is acquired at various times, we hope to contribute to the study of object detection and tracking in a diverse environment.</em></li>
          </ul>
        </div>
      </div>
    </div>

  </div>
</div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="https://github.com/sparolab/PoLaRIS" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
